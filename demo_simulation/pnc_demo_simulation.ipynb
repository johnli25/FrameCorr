{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaab9ec-ba7c-4e1f-bbd3-8377ffbf39e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffe5c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7682f046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm, trange\n",
    "import io\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2160d1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 17:35:52.374496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-12-05 17:35:52.412711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-12-05 17:35:52.412828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "  # Invalid device or cannot modify virtual devices once initialized.\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc3c1d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of imgs in the folder: 50000\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "img_folder = \"./val2017/\"\n",
    "img_paths = sorted(glob.glob(img_folder+'/*'))\n",
    "# print(img_paths)\n",
    "print(\"Number of imgs in the folder:\", len(img_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c401b04d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# label\n",
    "label_path = './data/ImageNetLabels.txt'\n",
    "with open(label_path, \"r\", encoding=\"UTF8\") as lbfile:\n",
    "    labels = lbfile.read().splitlines()\n",
    "\n",
    "# ground truths\n",
    "gt_path = './data/caffe_clsloc_validation_ground_truth.txt'\n",
    "with open(gt_path,\"r\") as lbfile:\n",
    "    lines = lbfile.readlines()\n",
    "    gts = []\n",
    "    for x in lines:\n",
    "        gts.append(int(x.split(' ')[1].splitlines()[0]))\n",
    "# gts = np.array(gts) + 1\n",
    "gts = np.array(gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03bf6e6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "model_folder = \"./image_classifiers\"\n",
    "model_name = \"efficientnet_b0_classification_1\" # https://tfhub.dev/tensorflow/efficientnet/b0/classification/1\n",
    "img_height,img_width = 224, 224\n",
    "model_path = os.path.join(model_folder, model_name)\n",
    "classifier = tf.keras.models.load_model(model_path)\n",
    "classifier._name = model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d08e8856",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import backend\n",
    "classifier.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),metrics=[tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name='top_5_accuracy')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1efd6e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d755efbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a31116a2",
   "metadata": {},
   "source": [
    "# Joint AE PNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe37fbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_folder = \"./joint_ae\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f706e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "encoder_mse = keras.models.load_model(model_save_folder + \"/best_model_save_encoder\")\n",
    "decoder_mse = keras.models.load_model(model_save_folder + \"/best_model_save_decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6eb4dcaf-2c5a-4a11-9388-d341d907c554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1\n",
    "filenames = tf.constant(img_paths)\n",
    "tf_labels = tf.constant(gts)\n",
    "\n",
    "# step 2: create a dataset returning slices of `filenames`\n",
    "dataset = tf.data.Dataset.from_tensor_slices((filenames, tf_labels))\n",
    "\n",
    "# step 3: parse every image in the dataset using `map`\n",
    "def _parse_function(filename, label):\n",
    "#     print(filename)\n",
    "    image_string = tf.io.read_file(filename)\n",
    "    image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
    "    image = tf.cast(image_decoded, tf.float32)\n",
    "    image /= 255.0\n",
    "    ##################Start AE##################\n",
    "#     image = encoderDecoder(image)\n",
    "    ##################End of AE##################\n",
    "    image = tf.image.resize(image, (img_height, img_width))\n",
    "#     image=tf.expand_dims(image,0)\n",
    "    return image, label\n",
    "\n",
    "dataset = dataset.map(_parse_function)\n",
    "# dataset = dataset.batch(32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc383d91-6ff4-4f9c-8eeb-135ee981c6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpxoenrydd/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpxoenrydd/assets\n",
      "/home/ruiqi/anaconda3/envs/edge_rateless/lib/python3.8/site-packages/tensorflow/lite/python/convert.py:746: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2023-12-05 17:43:15.087999: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2023-12-05 17:43:15.088014: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2023-12-05 17:43:15.088184: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpxoenrydd\n",
      "2023-12-05 17:43:15.088869: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2023-12-05 17:43:15.088878: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmpxoenrydd\n",
      "2023-12-05 17:43:15.090775: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2023-12-05 17:43:15.100789: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmpxoenrydd\n",
      "2023-12-05 17:43:15.105623: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 17439 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# representative_dataset\n",
    "def representative_dataset():\n",
    "  for data in dataset.batch(1).take(5000):\n",
    "#     print(data)\n",
    "    yield [tf.dtypes.cast(data[0], tf.float32)]\n",
    "\n",
    "# quantization settings\n",
    "converter_int8 = tf.lite.TFLiteConverter.from_keras_model(encoder_mse)\n",
    "converter_int8.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter_int8.representative_dataset = representative_dataset\n",
    "converter_int8.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter_int8.inference_input_type = tf.uint8  # or tf.uint8\n",
    "converter_int8.inference_output_type = tf.uint8  # or tf.uint8\n",
    "tflite_quant_model = converter_int8.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "665d6aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model.\n",
    "directory = './saved_tflite_models_demo/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "874db7ac-4217-4236-8b26-11c01f728f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "with open(directory+'best_encoder_tuned_model_uint8.tflite', 'wb') as f:\n",
    "  f.write(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5929ae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3e0357e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'dtype': <class 'numpy.uint8'>,\n",
      "  'index': 0,\n",
      "  'name': 'serving_default_input_3:0',\n",
      "  'quantization': (0.003921568859368563, 0),\n",
      "  'quantization_parameters': {'quantized_dimension': 0,\n",
      "                              'scales': array([0.00392157], dtype=float32),\n",
      "                              'zero_points': array([0], dtype=int32)},\n",
      "  'shape': array([  1, 224, 224,   3], dtype=int32),\n",
      "  'shape_signature': array([ -1, 224, 224,   3], dtype=int32),\n",
      "  'sparsity_parameters': {}}]\n",
      "[{'dtype': <class 'numpy.uint8'>,\n",
      "  'index': 8,\n",
      "  'name': 'StatefulPartitionedCall:0',\n",
      "  'quantization': (0.0212536808103323, 0),\n",
      "  'quantization_parameters': {'quantized_dimension': 0,\n",
      "                              'scales': array([0.02125368], dtype=float32),\n",
      "                              'zero_points': array([0], dtype=int32)},\n",
      "  'shape': array([ 1, 32, 32, 10], dtype=int32),\n",
      "  'shape_signature': array([-1, 32, 32, 10], dtype=int32),\n",
      "  'sparsity_parameters': {}}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=directory+'best_encoder_tuned_model_uint8.tflite', num_threads=12)\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "minp = interpreter.get_input_details()[0]['index']\n",
    "output_details = interpreter.get_output_details()\n",
    "mout=interpreter.get_output_details()[0]['index']\n",
    "# input details\n",
    "pprint(input_details)\n",
    "# output details\n",
    "pprint(output_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e6f9cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4fad476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qprint(msg):\n",
    "    \"\"\"Print right away\"\"\"\n",
    "    sys.stdout.write(msg + '\\n')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "def predict(img):\n",
    "    imsz = img.size\n",
    "    minsz = np.minimum(imsz[0], imsz[1])\n",
    "    imsz = (np.int16(list(imsz))-minsz)//2\n",
    "    crop = [imsz[0], imsz[1], imsz[0]+minsz, imsz[1]+minsz]\n",
    "\n",
    "    img = np.asarray(img.resize((img_height,img_width),\n",
    "#                                 box=crop,\n",
    "#                                 resample=Image.LANCZOS\n",
    "                               ),\n",
    "#                     dtype=\"float32\",\n",
    "                    )[np.newaxis, ...]\n",
    "#     print(img)\n",
    "#     img = (img).astype('uint8')\n",
    "#     img = img/255.0\n",
    "#     print(img)\n",
    "    interpreter.set_tensor(minp, img)\n",
    "    tict = time.time()\n",
    "    interpreter.invoke()\n",
    "    tict = time.time()-tict\n",
    "#     print(tict)\n",
    "    return interpreter.get_tensor(mout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8933ea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "quant_num_imgs = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a82d5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,de_h,de_w,de_c = output_details[0]['shape']\n",
    "quant_img_paths, quant_img_gts = img_paths[-quant_num_imgs:], gts[-quant_num_imgs:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24dcbbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_buffer = np.zeros((quant_num_imgs, de_h, de_w, de_c))\n",
    "# print(test_buffer.shape)\n",
    "# for idx, (cimg, cgt) in tqdm(enumerate(zip(quant_img_paths, quant_img_gts)), total=len(quant_img_paths)):\n",
    "#     with Image.open(cimg).convert('RGB') as im:\n",
    "#         im = im.resize((img_height,img_width))\n",
    "#         pr = predict(im)\n",
    "#         test_buffer[idx] = pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05fa3425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(os.path.join(directory, \"test_buffer\"), test_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0c8b8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_buffer = np.load(os.path.join(directory, \"test_buffer.npy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c84f701",
   "metadata": {},
   "source": [
    "## Further Quantize into Lower Bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a21244b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize(d, Q_level):\n",
    "    d_min, d_max = np.amin(d), np.amax(d)\n",
    "    d_min, d_max = 0,255\n",
    "    q = np.digitize(d, np.linspace(d_min, d_max, Q_level), right=True)\n",
    "    s = d_max - d_min\n",
    "    z = d_min\n",
    "    return q, s, z, Q_level\n",
    "\n",
    "\n",
    "def de_quantize(q, s, z, Q_level):\n",
    "    d = [e/Q_level*s+z for e in q]\n",
    "    return np.array(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b537db0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca8e8ff4",
   "metadata": {},
   "source": [
    "## Huffman Emcoding Added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d659c20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dahuffman import HuffmanCodec\n",
    "\n",
    "def quantize_embedding_with_customized_codec(embed, Q_level, codec):\n",
    "    ## Data flow:   embed -> quantize -> huffman encode -> huffman decode -> de_quantize\n",
    "    enc_q, s, z, _ = quantize(embed, Q_level)\n",
    "    \n",
    "    enc_huff = codec.encode([str(e) for e in enc_q.flatten()])\n",
    "    enc_huff_dec = [ int(x) for x in codec.decode(enc_huff)]\n",
    "    #print(\"Compression size before/after and error:\", len(embed), len(enc_huff), np.sum(np.abs(enc_huff_dec - enc_q)))\n",
    "\n",
    "    enc_q_dq = de_quantize(enc_huff_dec, s, z, Q_level).reshape(embed.shape)\n",
    "    #print(\"Mean quantization error and mean embed value:\", np.mean(np.abs(embed - enc_q_dq)), np.mean(np.abs(embed)))\n",
    "    return enc_q_dq, len(enc_huff)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d60a8e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_huffman_train_imgs = 10000\n",
    "# encoded_shape = output_details[0]['shape']\n",
    "# huffman_train_input = np.zeros((num_huffman_train_imgs,encoded_shape[1],encoded_shape[2],encoded_shape[3]))\n",
    "# for idx, huffman_train_img in tqdm(enumerate(img_paths[:num_huffman_train_imgs]), total=num_huffman_train_imgs):\n",
    "#     with Image.open(huffman_train_img).convert('RGB') as im:\n",
    "#         im = im.resize((img_height,img_width))\n",
    "#         pr = predict(im)\n",
    "\n",
    "#         huffman_train_input[idx] = pr\n",
    "# huffman_train_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8930c54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(os.path.join(directory, \"huffman_train_input_10000\"), huffman_train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c945776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "huffman_train_input = np.load(os.path.join(directory, \"huffman_train_input_10000.npy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08caadee",
   "metadata": {},
   "source": [
    "# Feat Huffman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03aebf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bit = 6\n",
    "pixel_range = 2 ** n_bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "174606c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate code table for 0\n",
      "(10240256,)\n",
      "[37 40 41 ... 63 63 63]\n",
      "Generate code table for 1\n",
      "(10240256,)\n",
      "[21 29 30 ... 63 63 63]\n",
      "Generate code table for 2\n",
      "(10240256,)\n",
      "[22 23 23 ... 63 63 63]\n",
      "Generate code table for 3\n",
      "(10240256,)\n",
      "[17 17 17 ... 63 63 63]\n",
      "Generate code table for 4\n",
      "(10240256,)\n",
      "[13 12 12 ... 63 63 63]\n",
      "Generate code table for 5\n",
      "(10240256,)\n",
      "[ 5  8  9 ... 63 63 63]\n",
      "Generate code table for 6\n",
      "(10240256,)\n",
      "[14  8  7 ... 63 63 63]\n",
      "Generate code table for 7\n",
      "(10240256,)\n",
      "[ 1  5  6 ... 63 63 63]\n",
      "Generate code table for 8\n",
      "(10240256,)\n",
      "[ 8  5  5 ... 63 63 63]\n",
      "Generate code table for 9\n",
      "(10240256,)\n",
      "[ 0  2  2 ... 63 63 63]\n"
     ]
    }
   ],
   "source": [
    "huffman_codec_list = []\n",
    "for i in range(10):\n",
    "    print(\"Generate code table for {}\".format(i))\n",
    "    training_list = (huffman_train_input[...,i].copy()).flatten()\n",
    "    training_list = np.hstack((training_list, np.arange(0,256)))\n",
    "    print(training_list.shape)\n",
    "    enc_q, s, z, _ = quantize(training_list, pixel_range)\n",
    "    print(enc_q)\n",
    "    codec = HuffmanCodec.from_data([str(e) for e in enc_q])\n",
    "    huffman_codec_list.append(codec)\n",
    "\n",
    "for i, codec in enumerate(huffman_codec_list):\n",
    "    codec.save(os.path.join(directory, 'huffman_table_fine_stoch_{}_bit_{:01d}'.format(n_bit, i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74926c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "huffman_codec_list = []\n",
    "for i in range(10):\n",
    "    codec = HuffmanCodec.load(os.path.join(directory, 'huffman_table_fine_stoch_{}_bit_{:01d}'.format(n_bit, i)))\n",
    "    huffman_codec_list.append(codec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb8b5cda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df0c0b21215e4bcbaed1dfa7a2bd9f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-01 17:29:52.724904: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8700\n",
      "2023-05-01 17:29:54.344014: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.     0.3815 0.4505 0.4515 0.6835 0.69   0.7465 0.767  0.79   0.8065\n",
      " 0.82   0.829  0.8335 0.8405 0.8495 0.855  0.8595 0.864  0.863  0.863\n",
      " 0.8625 0.8625 0.8625]\n"
     ]
    }
   ],
   "source": [
    "### Calculate accuracy at given size limits\n",
    "num_test_imgs = 2000\n",
    "k_list = [1,2,3,4,5,6,7,8,9,10]\n",
    "Ks = len(k_list)\n",
    "proposed_size_list = np.array(np.arange(0.5, 6.25, 0.25)) * 1024\n",
    "\n",
    "\n",
    "feature_size_table= np.zeros((num_test_imgs, Ks+1))\n",
    "feature_accuracy_table = np.zeros((num_test_imgs, Ks+1))\n",
    "\n",
    "cumulative_true = np.zeros(len(proposed_size_list))\n",
    "\n",
    "huffman_num_imgs = 10000\n",
    "huffman_img_paths, huffman_img_gts = img_paths[-huffman_num_imgs:], gts[-huffman_num_imgs:]\n",
    "\n",
    "for img_idx in tqdm(range(num_test_imgs)):\n",
    "    pr = np.copy(test_buffer[-huffman_num_imgs+img_idx])\n",
    "    gt_img = huffman_img_gts[-huffman_num_imgs+img_idx]\n",
    "    pr = pr.astype('uint8')\n",
    "    pr = pr[np.newaxis,...]\n",
    "    \n",
    "    pr_huffman = np.zeros_like(pr)\n",
    "    huffman_size = 0\n",
    "    pr_enc_q, s, z, _ = quantize(pr, pixel_range)\n",
    "    pr_enc_q_dq = de_quantize(pr_enc_q, s, z, pixel_range)\n",
    "    all_feature_stack = np.repeat(pr_enc_q_dq, Ks, axis=0)\n",
    "#     print(all_feature_stack.shape)\n",
    "    for j in range(Ks):\n",
    "        pr_flat = pr[:,:,:,j]\n",
    "        pr_huffman_feature, huffman_size_feature = quantize_embedding_with_customized_codec(pr_flat, pixel_range, huffman_codec_list[j])\n",
    "        pr[:,:,:,j] = pr_huffman_feature\n",
    "        huffman_size += huffman_size_feature\n",
    "        feature_size_table[img_idx, j+1] = huffman_size # size table update\n",
    "        all_feature_stack[j, :, :, j+1:] = 0\n",
    "    all_feature_stack = all_feature_stack * output_details[0]['quantization'][0]\n",
    "    decoded_result = decoder_mse.predict(all_feature_stack)\n",
    "\n",
    "    predictions = classifier.predict(decoded_result)\n",
    "#     pprint(predictions.shape)\n",
    "    \n",
    "    depth = predictions.shape[1]\n",
    "    gt_one_hot = tf.one_hot(gt_img, depth)[np.newaxis, ...]\n",
    "    gt_one_hot = np.repeat(gt_one_hot, Ks, axis=0)\n",
    "#     print(gt_img)\n",
    "    accuracy_top_five = tf.keras.metrics.top_k_categorical_accuracy(\n",
    "        gt_one_hot, predictions, k=5\n",
    "    )\n",
    "    feature_accuracy_table[img_idx, 1:] = accuracy_top_five\n",
    "    \n",
    "    sz_idx = np.searchsorted(feature_size_table[img_idx], proposed_size_list, side='left', sorter=None).astype(np.int32)\n",
    "\n",
    "    cumulative_true = cumulative_true + np.take(feature_accuracy_table[img_idx], sz_idx-1)\n",
    "#     print(cumulative_true)\n",
    "print(cumulative_true / num_test_imgs)\n",
    "\n",
    "np.save(os.path.join(directory, \"feature_accuracy_table_{}_bit\".format(n_bit)), feature_accuracy_table)\n",
    "np.save(os.path.join(directory, \"feature_size_table_{}_bit\".format(n_bit)), feature_size_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2a7d11ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "proposed_size_list = np.array(np.arange(0.75, 6, 0.25)) * 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4b7ea95-d0e4-4ef0-9086-87be21109cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.    , 0.3815, 0.4505, 0.4515, 0.6835, 0.69  , 0.7465, 0.767 ,\n",
       "       0.79  , 0.8065, 0.82  , 0.829 , 0.8335, 0.8405, 0.8495, 0.855 ,\n",
       "       0.8595, 0.864 , 0.863 , 0.863 , 0.8625, 0.8625, 0.8625])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cumulative_true/num_test_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56601b68-2892-40d5-b18c-9fd86e1683ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e4628e-11e5-484e-95ad-1b231ab438d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:edge_rateless]",
   "language": "python",
   "name": "conda-env-edge_rateless-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "844px",
    "left": "1852px",
    "right": "20px",
    "top": "108px",
    "width": "669px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
