{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ffe5c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "abbf171d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7682f046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm, trange\n",
    "import io\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2160d1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "  # Invalid device or cannot modify virtual devices once initialized.\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fc3c1d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of imgs in the folder: 5\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "img_folder = \"./val2017/\"\n",
    "img_paths = sorted(glob.glob(img_folder+'/*'))\n",
    "# print(img_paths)\n",
    "print(\"Number of imgs in the folder:\", len(img_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c401b04d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m     gts \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[0;32m---> 13\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     14\u001b[0m         gts\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mint\u001b[39m(x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplitlines()[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# gts = np.array(gts) + 1\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# label\n",
    "label_path = './data/ImageNetLabels.txt'\n",
    "with open(label_path, \"r\", encoding=\"UTF8\") as lbfile:\n",
    "    labels = lbfile.read().splitlines()\n",
    "\n",
    "# ground truths\n",
    "# gt_path = './data/caffe_clsloc_validation_ground_truth.txt'\n",
    "gt_path = './data/ILSVRC2012_validation_ground_truth.txt'\n",
    "with open(gt_path,\"r\") as lbfile:\n",
    "    lines = lbfile.readlines()\n",
    "    gts = []\n",
    "    for x in lines:\n",
    "        print(x.split(' ')[1])\n",
    "        gts.append(int(x.split(' ')[1].splitlines()[0]))\n",
    "# gts = np.array(gts) + 1\n",
    "gts = np.array(gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "03bf6e6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "model_folder = \"./image_classifiers\"\n",
    "model_name = \"efficientnet_b0_classification_1\" # https://tfhub.dev/tensorflow/efficientnet/b0/classification/1\n",
    "img_height,img_width = 224, 224\n",
    "model_path = os.path.join(model_folder, model_name)\n",
    "classifier = tf.keras.models.load_model(model_path)\n",
    "# classifier = keras.layers.TFSMLayer(model_path, call_endpoint=\"serving_default\")\n",
    "classifier._name = model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d08e8856",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import backend\n",
    "classifier.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),metrics=[tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name='top_5_accuracy')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1efd6e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d755efbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a31116a2",
   "metadata": {},
   "source": [
    "# Joint AE PNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fe37fbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_folder = \"./joint_ae\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f706e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer Orthogonal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "encoder_pnc = keras.models.load_model(model_save_folder + \"/best_model_save_encoder\")\n",
    "decoder_pnc = keras.models.load_model(model_save_folder + \"/best_model_save_decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6eb4dcaf-2c5a-4a11-9388-d341d907c554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 65 970 230 ... 232 982 355], shape=(50000,), dtype=int64)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimensions 5 and 50000 are not compatible",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(tf_labels)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# step 2: create a dataset returning slices of `filenames`\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# step 3: parse every image in the dataset using `map`\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_parse_function\u001b[39m(filename, label):\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#     print(filename)\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:825\u001b[0m, in \u001b[0;36mDatasetV2.from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# from_tensor_slices_op -> dataset_ops).\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_tensor_slices_op\n\u001b[0;32m--> 825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrom_tensor_slices_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py:25\u001b[0m, in \u001b[0;36m_from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_tensor_slices\u001b[39m(tensors, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 25\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_TensorSliceDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py:45\u001b[0m, in \u001b[0;36m_TensorSliceDataset.__init__\u001b[0;34m(self, element, is_files, name)\u001b[0m\n\u001b[1;32m     42\u001b[0m batch_dim \u001b[38;5;241m=\u001b[39m tensor_shape\u001b[38;5;241m.\u001b[39mDimension(\n\u001b[1;32m     43\u001b[0m     tensor_shape\u001b[38;5;241m.\u001b[39mdimension_value(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_shape()[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m---> 45\u001b[0m   \u001b[43mbatch_dim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_is_compatible_with\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtensor_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDimension\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m          \u001b[49m\u001b[43mtensor_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdimension_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m variant_tensor \u001b[38;5;241m=\u001b[39m gen_dataset_ops\u001b[38;5;241m.\u001b[39mtensor_slice_dataset(\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors,\n\u001b[1;32m     51\u001b[0m     output_shapes\u001b[38;5;241m=\u001b[39mstructure\u001b[38;5;241m.\u001b[39mget_flat_tensor_shapes(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_structure),\n\u001b[1;32m     52\u001b[0m     is_files\u001b[38;5;241m=\u001b[39mis_files,\n\u001b[1;32m     53\u001b[0m     metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\u001b[38;5;241m.\u001b[39mSerializeToString())\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(variant_tensor)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/framework/tensor_shape.py:303\u001b[0m, in \u001b[0;36mDimension.assert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Raises an exception if `other` is not compatible with this Dimension.\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \n\u001b[1;32m    295\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;124;03m    is_compatible_with).\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_compatible_with(other):\n\u001b[0;32m--> 303\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDimensions \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m are not compatible\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    304\u001b[0m                    (\u001b[38;5;28mself\u001b[39m, other))\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions 5 and 50000 are not compatible"
     ]
    }
   ],
   "source": [
    "# step 1\n",
    "filenames = tf.constant(img_paths)\n",
    "tf_labels = tf.constant(gts)\n",
    "\n",
    "# step 2: create a dataset returning slices of `filenames`\n",
    "dataset = tf.data.Dataset.from_tensor_slices((filenames, tf_labels))\n",
    "\n",
    "# step 3: parse every image in the dataset using `map`\n",
    "def _parse_function(filename, label):\n",
    "#     print(filename)\n",
    "    image_string = tf.io.read_file(filename)\n",
    "    image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
    "    image = tf.cast(image_decoded, tf.float32)\n",
    "    image /= 255.0\n",
    "    ##################Start AE##################\n",
    "#     image = encoderDecoder(image)\n",
    "    ##################End of AE##################\n",
    "    image = tf.image.resize(image, (img_height, img_width))\n",
    "#     image=tf.expand_dims(image,0)\n",
    "    return image, label\n",
    "\n",
    "dataset = dataset.map(_parse_function)\n",
    "# dataset = dataset.batch(32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc383d91-6ff4-4f9c-8eeb-135ee981c6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/th/8yt90_9d1375qgz708zr3hd80000gn/T/tmpu8gzb0qr/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/th/8yt90_9d1375qgz708zr3hd80000gn/T/tmpu8gzb0qr/assets\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "2024-02-27 13:09:26.645186: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-02-27 13:09:26.645199: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-02-27 13:09:26.645713: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/th/8yt90_9d1375qgz708zr3hd80000gn/T/tmpu8gzb0qr\n",
      "2024-02-27 13:09:26.646122: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-02-27 13:09:26.646127: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /var/folders/th/8yt90_9d1375qgz708zr3hd80000gn/T/tmpu8gzb0qr\n",
      "2024-02-27 13:09:26.646986: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "2024-02-27 13:09:26.647334: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-02-27 13:09:26.661020: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /var/folders/th/8yt90_9d1375qgz708zr3hd80000gn/T/tmpu8gzb0qr\n",
      "2024-02-27 13:09:26.665080: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 19368 microseconds.\n",
      "2024-02-27 13:09:26.673890: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 4, Total Ops 9, % non-converted = 44.44 %\n",
      " * 4 ARITH ops\n",
      "\n",
      "- arith.constant:    4 occurrences  (f32: 4)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 2)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m converter_int8\u001b[38;5;241m.\u001b[39minference_input_type \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39muint8  \u001b[38;5;66;03m# or tf.uint8\u001b[39;00m\n\u001b[1;32m     13\u001b[0m converter_int8\u001b[38;5;241m.\u001b[39minference_output_type \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39muint8  \u001b[38;5;66;03m# or tf.uint8\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m tflite_quant_model \u001b[38;5;241m=\u001b[39m \u001b[43mconverter_int8\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1139\u001b[0m, in \u001b[0;36m_export_metrics.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(convert_func)\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1138\u001b[0m   \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m-> 1139\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_and_export_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1093\u001b[0m, in \u001b[0;36mTFLiteConverterBase._convert_and_export_metrics\u001b[0;34m(self, convert_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_conversion_params_metric()\n\u001b[1;32m   1092\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mprocess_time()\n\u001b[0;32m-> 1093\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1094\u001b[0m elapsed_time_ms \u001b[38;5;241m=\u001b[39m (time\u001b[38;5;241m.\u001b[39mprocess_time() \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1601\u001b[0m, in \u001b[0;36mTFLiteKerasModelConverterV2.convert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;129m@_export_metrics\u001b[39m\n\u001b[1;32m   1589\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Converts a keras model based on instance variables.\u001b[39;00m\n\u001b[1;32m   1591\u001b[0m \n\u001b[1;32m   1592\u001b[0m \u001b[38;5;124;03m  Returns:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1599\u001b[0m \u001b[38;5;124;03m      Invalid quantization parameters.\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1601\u001b[0m   saved_model_convert_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_as_saved_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1602\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m saved_model_convert_result:\n\u001b[1;32m   1603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_convert_result\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1582\u001b[0m, in \u001b[0;36mTFLiteKerasModelConverterV2._convert_as_saved_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1578\u001b[0m   graph_def, input_tensors, output_tensors \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1579\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_keras_to_saved_model(temp_dir)\n\u001b[1;32m   1580\u001b[0m   )\n\u001b[1;32m   1581\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msaved_model_dir:\n\u001b[0;32m-> 1582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mTFLiteKerasModelConverterV2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgraph_def\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_tensors\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1586\u001b[0m   shutil\u001b[38;5;241m.\u001b[39mrmtree(temp_dir, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1378\u001b[0m, in \u001b[0;36mTFLiteConverterBaseV2.convert\u001b[0;34m(self, graph_def, input_tensors, output_tensors)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;66;03m# Converts model.\u001b[39;00m\n\u001b[1;32m   1371\u001b[0m result \u001b[38;5;241m=\u001b[39m _convert_graphdef(\n\u001b[1;32m   1372\u001b[0m     input_data\u001b[38;5;241m=\u001b[39mgraph_def,\n\u001b[1;32m   1373\u001b[0m     input_tensors\u001b[38;5;241m=\u001b[39minput_tensors,\n\u001b[1;32m   1374\u001b[0m     output_tensors\u001b[38;5;241m=\u001b[39moutput_tensors,\n\u001b[1;32m   1375\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconverter_kwargs,\n\u001b[1;32m   1376\u001b[0m )\n\u001b[0;32m-> 1378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimize_tflite_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_quant_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_io\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperimental_new_quantizer\u001b[49m\n\u001b[1;32m   1380\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py:215\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m    214\u001b[0m   report_error_message(\u001b[38;5;28mstr\u001b[39m(error))\n\u001b[0;32m--> 215\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m error \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py:205\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    204\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m ConverterError \u001b[38;5;28;01mas\u001b[39;00m converter_error:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m converter_error\u001b[38;5;241m.\u001b[39merrors:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1037\u001b[0m, in \u001b[0;36mTFLiteConverterBase._optimize_tflite_model\u001b[0;34m(self, model, quant_mode, quant_io)\u001b[0m\n\u001b[1;32m   1035\u001b[0m   q_allow_float \u001b[38;5;241m=\u001b[39m quant_mode\u001b[38;5;241m.\u001b[39mis_allow_float()\n\u001b[1;32m   1036\u001b[0m   q_variable_quantization \u001b[38;5;241m=\u001b[39m quant_mode\u001b[38;5;241m.\u001b[39menable_mlir_variable_quantization\n\u001b[0;32m-> 1037\u001b[0m   model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_quantize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m      \u001b[49m\u001b[43mq_in_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m      \u001b[49m\u001b[43mq_out_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m      \u001b[49m\u001b[43mq_activations_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m      \u001b[49m\u001b[43mq_bias_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m      \u001b[49m\u001b[43mq_allow_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m      \u001b[49m\u001b[43mq_variable_quantization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1047\u001b[0m m_in_type \u001b[38;5;241m=\u001b[39m in_type \u001b[38;5;28;01mif\u001b[39;00m in_type \u001b[38;5;28;01melse\u001b[39;00m _dtypes\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[1;32m   1048\u001b[0m m_out_type \u001b[38;5;241m=\u001b[39m out_type \u001b[38;5;28;01mif\u001b[39;00m out_type \u001b[38;5;28;01melse\u001b[39;00m _dtypes\u001b[38;5;241m.\u001b[39mfloat32\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:735\u001b[0m, in \u001b[0;36mTFLiteConverterBase._quantize\u001b[0;34m(self, result, input_type, output_type, activations_type, bias_type, allow_float, enable_variable_quantization)\u001b[0m\n\u001b[1;32m    731\u001b[0m calibrate_quantize \u001b[38;5;241m=\u001b[39m _calibrator\u001b[38;5;241m.\u001b[39mCalibrator(\n\u001b[1;32m    732\u001b[0m     result, custom_op_registerers_by_name, custom_op_registerers_by_func\n\u001b[1;32m    733\u001b[0m )\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experimental_calibrate_only \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_new_quantizer:\n\u001b[0;32m--> 735\u001b[0m   calibrated \u001b[38;5;241m=\u001b[39m \u001b[43mcalibrate_quantize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalibrate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresentative_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_gen\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experimental_calibrate_only:\n\u001b[1;32m    740\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m calibrated\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py:215\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m    214\u001b[0m   report_error_message(\u001b[38;5;28mstr\u001b[39m(error))\n\u001b[0;32m--> 215\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m error \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py:205\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    204\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m ConverterError \u001b[38;5;28;01mas\u001b[39;00m converter_error:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m converter_error\u001b[38;5;241m.\u001b[39merrors:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/lite/python/optimize/calibrator.py:254\u001b[0m, in \u001b[0;36mCalibrator.calibrate\u001b[0;34m(self, dataset_gen)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;129m@convert_phase\u001b[39m(Component\u001b[38;5;241m.\u001b[39mOPTIMIZE_TFLITE_MODEL, SubComponent\u001b[38;5;241m.\u001b[39mCALIBRATE)\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalibrate\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset_gen):\n\u001b[1;32m    246\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calibrates the model with specified generator.\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03m  Returns:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03m    dataset_gen: A generator that generates calibration samples.\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feed_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresize_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calibrator\u001b[38;5;241m.\u001b[39mCalibrate()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/lite/python/optimize/calibrator.py:101\u001b[0m, in \u001b[0;36mCalibrator._feed_tensors\u001b[0;34m(self, dataset_gen, resize_input)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Feed tensors to the calibrator.\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m initialized \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m dataset_gen():\n\u001b[1;32m    102\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sample, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sample[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m, in \u001b[0;36mrepresentative_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrepresentative_dataset\u001b[39m():\n\u001b[0;32m----> 3\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdataset\u001b[49m\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m5000\u001b[39m):\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#     print(data)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m [tf\u001b[38;5;241m.\u001b[39mdtypes\u001b[38;5;241m.\u001b[39mcast(data[\u001b[38;5;241m0\u001b[39m], tf\u001b[38;5;241m.\u001b[39mfloat32)]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# representative_dataset\n",
    "def representative_dataset():\n",
    "  for data in dataset.batch(1).take(5000):\n",
    "#     print(data)\n",
    "    yield [tf.dtypes.cast(data[0], tf.float32)]\n",
    "\n",
    "# quantization settings\n",
    "converter_int8 = tf.lite.TFLiteConverter.from_keras_model(encoder_pnc)\n",
    "converter_int8.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter_int8.representative_dataset = representative_dataset\n",
    "converter_int8.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter_int8.inference_input_type = tf.uint8  # or tf.uint8\n",
    "converter_int8.inference_output_type = tf.uint8  # or tf.uint8\n",
    "tflite_quant_model = converter_int8.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "665d6aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model.\n",
    "directory = './saved_tflite_models_demo/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "874db7ac-4217-4236-8b26-11c01f728f46",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tflite_quant_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(directory)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(directory\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_encoder_tuned_model_uint8.tflite\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 5\u001b[0m   f\u001b[38;5;241m.\u001b[39mwrite(\u001b[43mtflite_quant_model\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tflite_quant_model' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "with open(directory+'best_encoder_tuned_model_uint8.tflite', 'wb') as f:\n",
    "  f.write(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5929ae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3e0357e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'dtype': <class 'numpy.uint8'>,\n",
      "  'index': 0,\n",
      "  'name': 'serving_default_input_3:0',\n",
      "  'quantization': (0.003921568859368563, 0),\n",
      "  'quantization_parameters': {'quantized_dimension': 0,\n",
      "                              'scales': array([0.00392157], dtype=float32),\n",
      "                              'zero_points': array([0], dtype=int32)},\n",
      "  'shape': array([  1, 224, 224,   3], dtype=int32),\n",
      "  'shape_signature': array([ -1, 224, 224,   3], dtype=int32),\n",
      "  'sparsity_parameters': {}}]\n",
      "[{'dtype': <class 'numpy.uint8'>,\n",
      "  'index': 8,\n",
      "  'name': 'StatefulPartitionedCall:0',\n",
      "  'quantization': (0.0212536808103323, 0),\n",
      "  'quantization_parameters': {'quantized_dimension': 0,\n",
      "                              'scales': array([0.02125368], dtype=float32),\n",
      "                              'zero_points': array([0], dtype=int32)},\n",
      "  'shape': array([ 1, 32, 32, 10], dtype=int32),\n",
      "  'shape_signature': array([-1, 32, 32, 10], dtype=int32),\n",
      "  'sparsity_parameters': {}}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=directory+'best_encoder_tuned_model_uint8.tflite', num_threads=12)\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "minp = interpreter.get_input_details()[0]['index']\n",
    "output_details = interpreter.get_output_details()\n",
    "mout=interpreter.get_output_details()[0]['index']\n",
    "# input details\n",
    "pprint(input_details)\n",
    "# output details\n",
    "pprint(output_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e6f9cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4fad476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qprint(msg):\n",
    "    \"\"\"Print right away\"\"\"\n",
    "    sys.stdout.write(msg + '\\n')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "def predict(img):\n",
    "    imsz = img.size\n",
    "    minsz = np.minimum(imsz[0], imsz[1])\n",
    "    imsz = (np.int16(list(imsz))-minsz)//2\n",
    "    crop = [imsz[0], imsz[1], imsz[0]+minsz, imsz[1]+minsz]\n",
    "\n",
    "    img = np.asarray(img.resize((img_height,img_width),\n",
    "#                                 box=crop,\n",
    "#                                 resample=Image.LANCZOS\n",
    "                               ),\n",
    "#                     dtype=\"float32\",\n",
    "                    )[np.newaxis, ...]\n",
    "#     print(img)\n",
    "#     img = (img).astype('uint8')\n",
    "#     img = img/255.0\n",
    "#     print(img)\n",
    "    interpreter.set_tensor(minp, img)\n",
    "    tict = time.time()\n",
    "    interpreter.invoke()\n",
    "    tict = time.time()-tict\n",
    "#     print(tict)\n",
    "    return interpreter.get_tensor(mout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8933ea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "quant_num_imgs = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a82d5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,de_h,de_w,de_c = output_details[0]['shape']\n",
    "quant_img_paths, quant_img_gts = img_paths[-quant_num_imgs:], gts[-quant_num_imgs:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24dcbbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_buffer = np.zeros((quant_num_imgs, de_h, de_w, de_c))\n",
    "# print(test_buffer.shape)\n",
    "# for idx, (cimg, cgt) in tqdm(enumerate(zip(quant_img_paths, quant_img_gts)), total=len(quant_img_paths)):\n",
    "#     with Image.open(cimg).convert('RGB') as im:\n",
    "#         im = im.resize((img_height,img_width))\n",
    "#         pr = predict(im)\n",
    "#         test_buffer[idx] = pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05fa3425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(os.path.join(directory, \"test_buffer\"), test_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0c8b8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_buffer = np.load(os.path.join(directory, \"test_buffer.npy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c84f701",
   "metadata": {},
   "source": [
    "## Further Quantize into Lower Bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a21244b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize(d, Q_level):\n",
    "    d_min, d_max = np.amin(d), np.amax(d)\n",
    "    d_min, d_max = 0,255\n",
    "    q = np.digitize(d, np.linspace(d_min, d_max, Q_level), right=True)\n",
    "    s = d_max - d_min\n",
    "    z = d_min\n",
    "    return q, s, z, Q_level\n",
    "\n",
    "\n",
    "def de_quantize(q, s, z, Q_level):\n",
    "    d = [e/Q_level*s+z for e in q]\n",
    "    return np.array(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b537db0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca8e8ff4",
   "metadata": {},
   "source": [
    "## Huffman Emcoding Added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d659c20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dahuffman import HuffmanCodec\n",
    "\n",
    "def quantize_embedding_with_customized_codec(embed, Q_level, codec):\n",
    "    ## Data flow:   embed -> quantize -> huffman encode -> huffman decode -> de_quantize\n",
    "    enc_q, s, z, _ = quantize(embed, Q_level)\n",
    "    \n",
    "    enc_huff = codec.encode([str(e) for e in enc_q.flatten()])\n",
    "    enc_huff_dec = [ int(x) for x in codec.decode(enc_huff)]\n",
    "    #print(\"Compression size before/after and error:\", len(embed), len(enc_huff), np.sum(np.abs(enc_huff_dec - enc_q)))\n",
    "\n",
    "    enc_q_dq = de_quantize(enc_huff_dec, s, z, Q_level).reshape(embed.shape)\n",
    "    #print(\"Mean quantization error and mean embed value:\", np.mean(np.abs(embed - enc_q_dq)), np.mean(np.abs(embed)))\n",
    "    return enc_q_dq, len(enc_huff)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d60a8e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_huffman_train_imgs = 10000\n",
    "# encoded_shape = output_details[0]['shape']\n",
    "# huffman_train_input = np.zeros((num_huffman_train_imgs,encoded_shape[1],encoded_shape[2],encoded_shape[3]))\n",
    "# for idx, huffman_train_img in tqdm(enumerate(img_paths[:num_huffman_train_imgs]), total=num_huffman_train_imgs):\n",
    "#     with Image.open(huffman_train_img).convert('RGB') as im:\n",
    "#         im = im.resize((img_height,img_width))\n",
    "#         pr = predict(im)\n",
    "\n",
    "#         huffman_train_input[idx] = pr\n",
    "# huffman_train_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8930c54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(os.path.join(directory, \"huffman_train_input_10000\"), huffman_train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c945776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "huffman_train_input = np.load(os.path.join(directory, \"huffman_train_input_10000.npy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08caadee",
   "metadata": {},
   "source": [
    "# Feat Huffman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03aebf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bit = 6\n",
    "pixel_range = 2 ** n_bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "174606c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate code table for 0\n",
      "(10240256,)\n",
      "[37 40 41 ... 63 63 63]\n",
      "Generate code table for 1\n",
      "(10240256,)\n",
      "[21 29 30 ... 63 63 63]\n",
      "Generate code table for 2\n",
      "(10240256,)\n",
      "[22 23 23 ... 63 63 63]\n",
      "Generate code table for 3\n",
      "(10240256,)\n",
      "[17 17 17 ... 63 63 63]\n",
      "Generate code table for 4\n",
      "(10240256,)\n",
      "[13 12 12 ... 63 63 63]\n",
      "Generate code table for 5\n",
      "(10240256,)\n",
      "[ 5  8  9 ... 63 63 63]\n",
      "Generate code table for 6\n",
      "(10240256,)\n",
      "[14  8  7 ... 63 63 63]\n",
      "Generate code table for 7\n",
      "(10240256,)\n",
      "[ 1  5  6 ... 63 63 63]\n",
      "Generate code table for 8\n",
      "(10240256,)\n",
      "[ 8  5  5 ... 63 63 63]\n",
      "Generate code table for 9\n",
      "(10240256,)\n",
      "[ 0  2  2 ... 63 63 63]\n"
     ]
    }
   ],
   "source": [
    "huffman_codec_list = []\n",
    "for i in range(10):\n",
    "    print(\"Generate code table for {}\".format(i))\n",
    "    training_list = (huffman_train_input[...,i].copy()).flatten()\n",
    "    training_list = np.hstack((training_list, np.arange(0,256)))\n",
    "    print(training_list.shape)\n",
    "    enc_q, s, z, _ = quantize(training_list, pixel_range)\n",
    "    print(enc_q)\n",
    "    codec = HuffmanCodec.from_data([str(e) for e in enc_q])\n",
    "    huffman_codec_list.append(codec)\n",
    "\n",
    "for i, codec in enumerate(huffman_codec_list):\n",
    "    codec.save(os.path.join(directory, 'huffman_table_fine_stoch_{}_bit_{:01d}'.format(n_bit, i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74926c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "huffman_codec_list = []\n",
    "for i in range(10):\n",
    "    codec = HuffmanCodec.load(os.path.join(directory, 'huffman_table_fine_stoch_{}_bit_{:01d}'.format(n_bit, i)))\n",
    "    huffman_codec_list.append(codec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb8b5cda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m k_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m7\u001b[39m,\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m9\u001b[39m,\u001b[38;5;241m10\u001b[39m]\n\u001b[1;32m      4\u001b[0m Ks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(k_list)\n\u001b[0;32m----> 5\u001b[0m proposed_size_list \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marray(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m6.25\u001b[39m, \u001b[38;5;241m0.25\u001b[39m)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m\n\u001b[1;32m      8\u001b[0m feature_size_table\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((num_test_imgs, Ks\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m      9\u001b[0m feature_accuracy_table \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((num_test_imgs, Ks\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "### Calculate accuracy at given size limits\n",
    "num_test_imgs = 2000\n",
    "k_list = [1,2,3,4,5,6,7,8,9,10]\n",
    "Ks = len(k_list)\n",
    "proposed_size_list = np.array(np.arange(0.5, 6.25, 0.25)) * 1024\n",
    "\n",
    "\n",
    "feature_size_table= np.zeros((num_test_imgs, Ks+1))\n",
    "feature_accuracy_table = np.zeros((num_test_imgs, Ks+1))\n",
    "\n",
    "cumulative_true = np.zeros(len(proposed_size_list))\n",
    "\n",
    "huffman_num_imgs = 10000\n",
    "huffman_img_paths, huffman_img_gts = img_paths[-huffman_num_imgs:], gts[-huffman_num_imgs:]\n",
    "\n",
    "for img_idx in tqdm(range(num_test_imgs)):\n",
    "    pr = np.copy(test_buffer[-huffman_num_imgs+img_idx])\n",
    "    gt_img = huffman_img_gts[-huffman_num_imgs+img_idx]\n",
    "    pr = pr.astype('uint8')\n",
    "    pr = pr[np.newaxis,...]\n",
    "    \n",
    "    pr_huffman = np.zeros_like(pr)\n",
    "    huffman_size = 0\n",
    "    pr_enc_q, s, z, _ = quantize(pr, pixel_range)\n",
    "    pr_enc_q_dq = de_quantize(pr_enc_q, s, z, pixel_range)\n",
    "    all_feature_stack = np.repeat(pr_enc_q_dq, Ks, axis=0)\n",
    "#     print(all_feature_stack.shape)\n",
    "    for j in range(Ks):\n",
    "        pr_flat = pr[:,:,:,j]\n",
    "        pr_huffman_feature, huffman_size_feature = quantize_embedding_with_customized_codec(pr_flat, pixel_range, huffman_codec_list[j])\n",
    "        pr[:,:,:,j] = pr_huffman_feature\n",
    "        huffman_size += huffman_size_feature\n",
    "        feature_size_table[img_idx, j+1] = huffman_size # size table update\n",
    "        all_feature_stack[j, :, :, j+1:] = 0\n",
    "    all_feature_stack = all_feature_stack * output_details[0]['quantization'][0]\n",
    "    decoded_result = decoder_pnc.predict(all_feature_stack)\n",
    "\n",
    "    predictions = classifier.predict(decoded_result)\n",
    "#     pprint(predictions.shape)\n",
    "    \n",
    "    depth = predictions.shape[1]\n",
    "    gt_one_hot = tf.one_hot(gt_img, depth)[np.newaxis, ...]\n",
    "    gt_one_hot = np.repeat(gt_one_hot, Ks, axis=0)\n",
    "#     print(gt_img)\n",
    "    accuracy_top_five = tf.keras.metrics.top_k_categorical_accuracy(\n",
    "        gt_one_hot, predictions, k=5\n",
    "    )\n",
    "    feature_accuracy_table[img_idx, 1:] = accuracy_top_five\n",
    "    \n",
    "    sz_idx = np.searchsorted(feature_size_table[img_idx], proposed_size_list, side='left', sorter=None).astype(np.int32)\n",
    "\n",
    "    cumulative_true = cumulative_true + np.take(feature_accuracy_table[img_idx], sz_idx-1)\n",
    "#     print(cumulative_true)\n",
    "print(cumulative_true / num_test_imgs)\n",
    "\n",
    "np.save(os.path.join(directory, \"feature_accuracy_table_{}_bit\".format(n_bit)), feature_accuracy_table)\n",
    "np.save(os.path.join(directory, \"feature_size_table_{}_bit\".format(n_bit)), feature_size_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2a7d11ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "proposed_size_list = np.array(np.arange(0.75, 6, 0.25)) * 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4b7ea95-d0e4-4ef0-9086-87be21109cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.    , 0.3815, 0.4505, 0.4515, 0.6835, 0.69  , 0.7465, 0.767 ,\n",
       "       0.79  , 0.8065, 0.82  , 0.829 , 0.8335, 0.8405, 0.8495, 0.855 ,\n",
       "       0.8595, 0.864 , 0.863 , 0.863 , 0.8625, 0.8625, 0.8625])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cumulative_true/num_test_imgs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "844px",
    "left": "1852px",
    "right": "20px",
    "top": "108px",
    "width": "669px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
