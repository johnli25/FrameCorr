{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffe5c1d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e371012-48ed-4082-a287-17aecbabb58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2160d1dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has GPU\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "  # Invalid device or cannot modify virtual devices once initialized.\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "157fe43a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_save_folder = \"./joint_ae\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc3c1d8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of imgs in the folder: 5\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "img_folder = \"./val2017/\"\n",
    "img_paths = sorted(glob.glob(img_folder+'/*'))\n",
    "# print(img_paths)\n",
    "print(\"Number of imgs in the folder:\", len(img_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c401b04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label\n",
    "label_path = './data/ImageNetLabels.txt'\n",
    "with open(label_path, \"r\", encoding=\"UTF8\") as lbfile:\n",
    "    labels = lbfile.read().splitlines()\n",
    "\n",
    "# ground truths\n",
    "gt_path = './data/caffe_clsloc_validation_ground_truth.txt'\n",
    "with open(gt_path,\"r\") as lbfile:\n",
    "    lines = lbfile.readlines()\n",
    "    gts = []\n",
    "    for x in lines:\n",
    "        gts.append(int(x.split(' ')[1].splitlines()[0]))\n",
    "# gts = np.array(gts) + 1\n",
    "gts = np.array(gts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c22d58d",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db98e7fe",
   "metadata": {},
   "source": [
    "### Regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6207b183",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "C = 1e-4\n",
    "\n",
    "def orthogonal_reg(w):  # 1703.01827\n",
    "  units = w.shape[-1]\n",
    "  w = tf.reshape(w, (-1, units))\n",
    "  w = tf.transpose(w) @ w\n",
    "  \n",
    "  return (C/2)*tf.linalg.norm(w - tf.eye(units))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fe1c0a",
   "metadata": {},
   "source": [
    "### Prepare Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de545cfc-c306-4031-af36-dbc427a3436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height, img_width = 224,224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f4c8631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224 224\n",
      "Model: \"enocder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 32, 32, 16)        3904      \n",
      "                                                                 \n",
      " encoder_out (Conv2D)        (None, 32, 32, 10)        1450      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5354 (20.91 KB)\n",
      "Trainable params: 5354 (20.91 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(img_height, img_width)\n",
    "\n",
    "encoder_input = layers.Input(shape=(img_height, img_width, 3))\n",
    "\n",
    "# Encoder\n",
    "\n",
    "initializer = tf.keras.initializers.Orthogonal()\n",
    "encoder_x = layers.Conv2D(16, (9, 9), \n",
    "                strides=7, \n",
    "                activation=\"relu\", \n",
    "                padding=\"same\", \n",
    "                kernel_initializer=initializer\n",
    "                )(encoder_input)\n",
    "\n",
    "encoder_x = layers.Conv2D(10, (3, 3), \n",
    "                strides=1,\n",
    "                activation=\"relu\", \n",
    "                padding=\"same\", \n",
    "                kernel_initializer=initializer,\n",
    "                name='encoder_out'\n",
    "                )(encoder_x)\n",
    "\n",
    "encoder_model = keras.Model(encoder_input, encoder_x,  name='enocder')\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "481fe471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_tail(X):\n",
    "    total_dim = tf.shape(X)[-1]\n",
    "    tail_len = tf.random.uniform([1,], minval=0, maxval=total_dim, dtype=tf.int32)\n",
    "    tail_len = tf.math.minimum(tail_len, total_dim)\n",
    "    head_len = total_dim - tail_len\n",
    "    mask = tf.concat((tf.ones([tf.shape(X)[1], tf.shape(X)[2], head_len[0]]), tf.zeros((tf.shape(X)[1], tf.shape(X)[2],tail_len[0]))), axis=-1)\n",
    "    X = X*mask\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8af3b52a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_8 (InputLayer)        [(None, 32, 32, 10)]         0         []                            \n",
      "                                                                                                  \n",
      " decoder_input (Conv2DTrans  (None, 224, 224, 64)         51904     ['input_8[0][0]']             \n",
      " pose)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)          (None, 224, 224, 64)         102464    ['decoder_input[0][0]']       \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TF  (None, 224, 224, 64)         0         ['conv2d_11[0][0]',           \n",
      " OpLambda)                                                           'decoder_input[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)          (None, 224, 224, 64)         102464    ['tf.__operators__.add_4[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)          (None, 224, 224, 64)         102464    ['conv2d_12[0][0]']           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TF  (None, 224, 224, 64)         0         ['conv2d_13[0][0]',           \n",
      " OpLambda)                                                           'conv2d_12[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)          (None, 224, 224, 3)          1731      ['tf.__operators__.add_5[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.clip_by_value_2 (TFOpLa  (None, 224, 224, 3)          0         ['conv2d_14[0][0]']           \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 361027 (1.38 MB)\n",
      "Trainable params: 361027 (1.38 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Decoder\n",
    "_,w,h,c = encoder_model.get_layer('encoder_out').output_shape\n",
    "decoder_input = layers.Input(shape=(w,h,c))\n",
    "decoder_x = layers.Conv2DTranspose(64, (9, 9), \n",
    "                                strides=7, \n",
    "                                activation=\"relu\", \n",
    "                                padding=\"same\",\n",
    "                                name=\"decoder_input\"\n",
    "                                )(decoder_input)\n",
    "decoder_x = layers.Conv2D(64, (5, 5), strides=1, activation=\"relu\",padding=\"same\")(decoder_x) + decoder_x\n",
    "\n",
    "decoder_x = layers.Conv2D(64, (5, 5), strides=1, activation=\"relu\",padding=\"same\")(decoder_x)\n",
    "\n",
    "decoder_x = layers.Conv2D(64, (5, 5), strides=1, activation=\"relu\",padding=\"same\")(decoder_x) + decoder_x\n",
    "\n",
    "decoder_out7 = layers.Conv2D(3, (3, 3),  padding=\"same\")(decoder_x)\n",
    "decoder_out7 = tf.clip_by_value(decoder_out7, clip_value_min=0, clip_value_max=1)  \n",
    "\n",
    "# Autoencoder\n",
    "decoder_model = keras.Model(decoder_input, decoder_out7,  name='decoder')\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac889e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1658253d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainStep(tf.keras.Model):\n",
    "    def __init__(self, n_gradients, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.n_gradients = tf.constant(n_gradients, dtype=tf.int32)\n",
    "        self.n_acum_step = tf.Variable(0, dtype=tf.int32, trainable=False)\n",
    "        self.gradient_accumulation = [tf.Variable(tf.zeros_like(v, dtype=tf.float32), trainable=False) for v in self.trainable_variables]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        self.n_acum_step.assign_add(1)\n",
    "\n",
    "        x, y = data\n",
    "        # Gradient Tape\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)\n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "        # Calculate batch gradients\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        # Accumulate batch gradients\n",
    "        for i in range(len(self.gradient_accumulation)):\n",
    "            self.gradient_accumulation[i].assign_add(gradients[i])\n",
    " \n",
    "        # If n_acum_step reach the n_gradients then we apply accumulated gradients to update the variables otherwise do nothing\n",
    "        tf.cond(tf.equal(self.n_acum_step, self.n_gradients), self.apply_accu_gradients, lambda: None)\n",
    "\n",
    "        # update metrics\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def apply_accu_gradients(self):\n",
    "        # apply accumulated gradients\n",
    "        self.optimizer.apply_gradients(zip(self.gradient_accumulation, self.trainable_variables))\n",
    "\n",
    "        # reset\n",
    "        self.n_acum_step.assign(0)\n",
    "        for i in range(len(self.gradient_accumulation)):\n",
    "            self.gradient_accumulation[i].assign(tf.zeros_like(self.trainable_variables[i], dtype=tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9aa9744c-3e94-438a-8093-c62a559760dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x2900a3f10>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_model.load_weights(model_save_folder + \"/best_model_save_encoder/variables/variables\")\n",
    "encoder_model.load_weights(model_save_folder + \"/best_model_save_decoder/variables/variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4732c2cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ae_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)        [(None, 224, 224, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " enocder (Functional)        (None, 32, 32, 10)           5354      ['input_9[0][0]']             \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape_10 (TFO  (4,)                         0         ['enocder[2][0]']             \n",
      " pLambda)                                                                                         \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1  ()                           0         ['tf.compat.v1.shape_10[0][0]'\n",
      " 4 (SlicingOpLambda)                                                ]                             \n",
      "                                                                                                  \n",
      " tf.random.uniform_2 (TFOpL  (1,)                         0         ['tf.__operators__.getitem_14[\n",
      " ambda)                                                             0][0]']                       \n",
      "                                                                                                  \n",
      " tf.math.minimum_2 (TFOpLam  (1,)                         0         ['tf.random.uniform_2[0][0]', \n",
      " bda)                                                                'tf.__operators__.getitem_14[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape_11 (TFO  (4,)                         0         ['enocder[2][0]']             \n",
      " pLambda)                                                                                         \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape_12 (TFO  (4,)                         0         ['enocder[2][0]']             \n",
      " pLambda)                                                                                         \n",
      "                                                                                                  \n",
      " tf.math.subtract_2 (TFOpLa  (1,)                         0         ['tf.__operators__.getitem_14[\n",
      " mbda)                                                              0][0]',                       \n",
      "                                                                     'tf.math.minimum_2[0][0]']   \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape_13 (TFO  (4,)                         0         ['enocder[2][0]']             \n",
      " pLambda)                                                                                         \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape_14 (TFO  (4,)                         0         ['enocder[2][0]']             \n",
      " pLambda)                                                                                         \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1  ()                           0         ['tf.compat.v1.shape_11[0][0]'\n",
      " 5 (SlicingOpLambda)                                                ]                             \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1  ()                           0         ['tf.compat.v1.shape_12[0][0]'\n",
      " 6 (SlicingOpLambda)                                                ]                             \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1  ()                           0         ['tf.math.subtract_2[0][0]']  \n",
      " 7 (SlicingOpLambda)                                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1  ()                           0         ['tf.compat.v1.shape_13[0][0]'\n",
      " 8 (SlicingOpLambda)                                                ]                             \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1  ()                           0         ['tf.compat.v1.shape_14[0][0]'\n",
      " 9 (SlicingOpLambda)                                                ]                             \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_2  ()                           0         ['tf.math.minimum_2[0][0]']   \n",
      " 0 (SlicingOpLambda)                                                                              \n",
      "                                                                                                  \n",
      " tf.ones_2 (TFOpLambda)      (32, 32, None)               0         ['tf.__operators__.getitem_15[\n",
      "                                                                    0][0]',                       \n",
      "                                                                     'tf.__operators__.getitem_16[\n",
      "                                                                    0][0]',                       \n",
      "                                                                     'tf.__operators__.getitem_17[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " tf.zeros_2 (TFOpLambda)     (32, 32, None)               0         ['tf.__operators__.getitem_18[\n",
      "                                                                    0][0]',                       \n",
      "                                                                     'tf.__operators__.getitem_19[\n",
      "                                                                    0][0]',                       \n",
      "                                                                     'tf.__operators__.getitem_20[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " tf.concat_2 (TFOpLambda)    (32, 32, None)               0         ['tf.ones_2[0][0]',           \n",
      "                                                                     'tf.zeros_2[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.multiply_2 (TFOpLa  (None, 32, 32, 10)           0         ['enocder[2][0]',             \n",
      " mbda)                                                               'tf.concat_2[0][0]']         \n",
      "                                                                                                  \n",
      " decoder (Functional)        (None, 224, 224, 3)          361027    ['tf.math.multiply_2[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 366381 (1.40 MB)\n",
      "Trainable params: 366381 (1.40 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_ae = layers.Input(shape=(img_height, img_width, 3))\n",
    "\n",
    "e_out = encoder_model(input_ae)\n",
    "e_out = dropout_tail(e_out)\n",
    "d_out = decoder_model(e_out)\n",
    "\n",
    "autoencoder_model = keras.Model(inputs=[input_ae], outputs=[d_out], name=\"ae_model\")\n",
    "# autoencoder_model = CustomTrainStep(n_gradients=5, inputs=[input_ae], outputs=[d_out], name=\"ae_model\")\n",
    "autoencoder_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mse')\n",
    "autoencoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66770134",
   "metadata": {},
   "source": [
    "## Split the Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b8251359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cut_encoder_decoder(autoencoder_model, layerName = \"decoder\", verbose=False):\n",
    "    decoder_input_index = None\n",
    "#     layerName = layerName\n",
    "    for idx, layer in enumerate(autoencoder_model.layers):\n",
    "        if layer.name == layerName:\n",
    "            decoder_input_index = idx\n",
    "            break\n",
    "\n",
    "    if verbose: print(\"Decoder index:\", decoder_input_index,\"\\n---\")\n",
    "\n",
    "    # encoder = keras.Model(autoencoder_tail_model.get_layer(\"input_4\").input, autoencoder_tail_model.get_layer(\"encoder\").output, name='encoder1')\n",
    "\n",
    "    encoder = tf.keras.Sequential(name='encoder1')\n",
    "    for layer in autoencoder_model.layers[:2]:\n",
    "        encoder.add(layer)\n",
    "\n",
    "    # encoder.compile()\n",
    "    if verbose: encoder.summary()\n",
    "\n",
    "    decoder = tf.keras.Sequential(name='decoder1')\n",
    "    for layer in autoencoder_model.layers[decoder_input_index:]:\n",
    "        decoder.add(layer)\n",
    "\n",
    "\n",
    "    # encoder.compile()\n",
    "    if verbose: decoder.summary()\n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a7c3360c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder index: 21 \n",
      "---\n",
      "Model: \"encoder1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " enocder (Functional)        (None, 32, 32, 10)        5354      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5354 (20.91 KB)\n",
      "Trainable params: 5354 (20.91 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"decoder1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder (Functional)        (None, 224, 224, 3)       361027    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 361027 (1.38 MB)\n",
      "Trainable params: 361027 (1.38 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_pnc, decoder_pnc = cut_encoder_decoder(autoencoder_model, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "844px",
    "left": "1852px",
    "right": "20px",
    "top": "108px",
    "width": "669px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
