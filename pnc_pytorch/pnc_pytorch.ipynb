{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Custom Orthogonal Regularizer\n",
    "C = 1e-4\n",
    "\n",
    "def orthogonal_reg(w):\n",
    "    units = w.shape[-1]\n",
    "    w = w.view(-1, units)\n",
    "    eye = torch.eye(units, device=w.device)\n",
    "    return (C / 2) * torch.norm(w.T @ w - eye)\n",
    "\n",
    "# Dropout tail function\n",
    "def dropout_tail(X):\n",
    "    total_dim = X.shape[-1]\n",
    "    tail_len = torch.randint(0, total_dim + 1, (1,))\n",
    "    head_len = total_dim - tail_len\n",
    "    mask = torch.cat((torch.ones(X.shape[1], X.shape[2], head_len.item()), \n",
    "                      torch.zeros(X.shape[1], X.shape[2], tail_len.item())), dim=-1).to(X.device)\n",
    "    return X * mask\n",
    "\n",
    "# Encoder Model\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=9, stride=7, padding=4)\n",
    "        self.conv2 = nn.Conv2d(16, 10, kernel_size=3, stride=1, padding=1)\n",
    "        nn.init.orthogonal_(self.conv1.weight)\n",
    "        nn.init.orthogonal_(self.conv2.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        return x\n",
    "\n",
    "# Decoder Model\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv_transpose1 = nn.ConvTranspose2d(10, 64, kernel_size=9, stride=7, padding=4, output_padding=6 )\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv4 = nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv_transpose1(x))\n",
    "        x = F.relu(self.conv2(x)) + x\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv3(x)) + x\n",
    "        x = self.conv4(x)\n",
    "        x = torch.clamp(x, min=0, max=1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Autoencoder Model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder(\n",
      "  (encoder): Encoder(\n",
      "    (conv1): Conv2d(3, 16, kernel_size=(9, 9), stride=(7, 7), padding=(4, 4))\n",
      "    (conv2): Conv2d(16, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (conv_transpose1): ConvTranspose2d(10, 64, kernel_size=(9, 9), stride=(7, 7), padding=(4, 4))\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (conv3): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (conv4): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "autoencoder = Autoencoder()\n",
    "print(autoencoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder model loaded from: joint_ae/best_model_save_encoder\n",
      "Encoder model variables:\n",
      "  conv2d/kernel:0: (9, 9, 3, 16)\n",
      "  conv2d/bias:0: (16,)\n",
      "  encoder_out/kernel:0: (3, 3, 16, 10)\n",
      "  encoder_out/bias:0: (10,)\n",
      "\n",
      "Decoder model loaded from: joint_ae/best_model_save_decoder\n",
      "Decoder model variables:\n",
      "  decoder_input/kernel:0: (9, 9, 64, 10)\n",
      "  decoder_input/bias:0: (64,)\n",
      "  conv2d_1/kernel:0: (5, 5, 64, 64)\n",
      "  conv2d_1/bias:0: (64,)\n",
      "  conv2d_2/kernel:0: (5, 5, 64, 64)\n",
      "  conv2d_2/bias:0: (64,)\n",
      "  conv2d_3/kernel:0: (5, 5, 64, 64)\n",
      "  conv2d_3/bias:0: (64,)\n",
      "  conv2d_4/kernel:0: (3, 3, 64, 3)\n",
      "  conv2d_4/bias:0: (3,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import tensorflow as tf\n",
    "\n",
    "# Define paths for the encoder and decoder models\n",
    "encoder_model_dir = \"joint_ae/best_model_save_encoder\"\n",
    "decoder_model_dir = \"joint_ae/best_model_save_decoder\"\n",
    "\n",
    "# Load the encoder model and inspect variables\n",
    "tf_encoder_model = tf.saved_model.load(encoder_model_dir)\n",
    "print(\"Encoder model loaded from:\", encoder_model_dir)\n",
    "print(\"Encoder model variables:\")\n",
    "for var in tf_encoder_model.variables:\n",
    "    print(f\"  {var.name}: {var.shape}\")\n",
    "\n",
    "# Load the decoder model and inspect variables\n",
    "tf_decoder_model = tf.saved_model.load(decoder_model_dir)\n",
    "print(\"\\nDecoder model loaded from:\", decoder_model_dir)\n",
    "print(\"Decoder model variables:\")\n",
    "for var in tf_decoder_model.variables:\n",
    "    print(f\"  {var.name}: {var.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights loaded successfully into autoencoder.encoder and autoencoder.decoder.\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store TensorFlow variables by name for easier lookup for `loaded`\n",
    "tf_weights_encoder = {v.name: v.numpy() for v in tf_encoder_model.variables}\n",
    "\n",
    "# Load weights for the Encoder from `tf_weights_encoder`\n",
    "with torch.no_grad():\n",
    "    # Load weights and biases for conv1\n",
    "    autoencoder.encoder.conv1.weight.copy_(torch.tensor(tf_weights_encoder['conv2d/kernel:0']).permute(3, 2, 0, 1))\n",
    "    autoencoder.encoder.conv1.bias.copy_(torch.tensor(tf_weights_encoder['conv2d/bias:0']))\n",
    "\n",
    "    # Load weights and biases for conv2\n",
    "    autoencoder.encoder.conv2.weight.copy_(torch.tensor(tf_weights_encoder['encoder_out/kernel:0']).permute(3, 2, 0, 1))\n",
    "    autoencoder.encoder.conv2.bias.copy_(torch.tensor(tf_weights_encoder['encoder_out/bias:0']))\n",
    "\n",
    "# Dictionary to store `loaded2` TensorFlow variables by name for the decoder part\n",
    "tf_weights_decoder = {v.name: v.numpy() for v in tf_decoder_model.variables}\n",
    "\n",
    "# Load weights for the Decoder from `tf_weights_decoder`\n",
    "with torch.no_grad():\n",
    "    # Load weights and biases for conv_transpose1\n",
    "    autoencoder.decoder.conv_transpose1.weight.copy_(torch.tensor(tf_weights_decoder['decoder_input/kernel:0']).permute(3, 2, 0, 1))\n",
    "    autoencoder.decoder.conv_transpose1.bias.copy_(torch.tensor(tf_weights_decoder['decoder_input/bias:0']))\n",
    "\n",
    "    # Load weights and biases for conv2\n",
    "    autoencoder.decoder.conv2.weight.copy_(torch.tensor(tf_weights_decoder['conv2d_1/kernel:0']).permute(3, 2, 0, 1))\n",
    "    autoencoder.decoder.conv2.bias.copy_(torch.tensor(tf_weights_decoder['conv2d_1/bias:0']))\n",
    "\n",
    "    # Load weights and biases for conv3\n",
    "    autoencoder.decoder.conv3.weight.copy_(torch.tensor(tf_weights_decoder['conv2d_2/kernel:0']).permute(3, 2, 0, 1))\n",
    "    autoencoder.decoder.conv3.bias.copy_(torch.tensor(tf_weights_decoder['conv2d_2/bias:0']))\n",
    "\n",
    "    # Load weights and biases for conv4\n",
    "    autoencoder.decoder.conv4.weight.copy_(torch.tensor(tf_weights_decoder['conv2d_4/kernel:0']).permute(3, 2, 0, 1))\n",
    "    autoencoder.decoder.conv4.bias.copy_(torch.tensor(tf_weights_decoder['conv2d_4/bias:0']))\n",
    "\n",
    "print(\"Weights loaded successfully into autoencoder.encoder and autoencoder.decoder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(autoencoder.state_dict(), 'pnc_autoencoder_weights.pth')\n",
    "torch.save(autoencoder.encoder.state_dict(), 'pnc_encoder_weights.pth')\n",
    "torch.save(autoencoder.decoder.state_dict(), 'pnc_decoder_weights.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_cell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
